{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Tune_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Tune Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparamters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forgot to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjgZlEazcF70",
        "outputId": "819cc4ea-7351-4635-a3d3-6c34ba55879b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U keras-tuner"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=6665349b22e5eb9826ee41936bb597f1e3602a6d74a33bcfc6de380409fc02d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=9bdcfe0d0fc0dcfb523c359731fe4ea3de947870d190c051629663e76d64d077\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNrDK0AQb8-C"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0z98Khmb8-D"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB_LiZMub8-E"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyH2K0erb8-E",
        "outputId": "0779d59a-3f93-48a6-d369-0c9c59ed885b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glpeAc2zb8-E",
        "outputId": "a53fedb4-3f86-476c-c857-3b65e1d13b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYzmfvAHb8-F"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV \n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a complied keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in model \n",
        "        To be clear, this excludes the input and output layer.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model \n",
        "    return model\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EmGZvP9b8-H"
      },
      "source": [
        "## Explore create_model\n",
        "\n",
        "Let's build a few different models in order to understand how the above code works in practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-OTk3-ib8-H"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "z_erWJ27b8-H"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct=\"relu\", negative_node_incrementation=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0SLXlX0b8-I",
        "outputId": "6d621446-781c-4e3f-a115-8a7a5e59c967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 456)               228456    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 412)               188284    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 367)               151571    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 323)               118864    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 278)               90072     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 234)               65286     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 189)               44415     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 145)               27550     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1460      \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXkqaCS1b8-I"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e0722533c325d699f4842e874e43720e",
          "grade": false,
          "grade_id": "cell-99d563a291231a7b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "qmMDF5JZb8-I"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct=\"relu\", negative_node_incrementation=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L49XLkdqb8-J",
        "outputId": "e591923b-3eb9-484a-b6fe-e8a5eaa63a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
        "# The output layer must have 10 nodes because there are 10 labels to predict \n",
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 545)               273045    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 589)               321594    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 634)               374060    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 678)               430530    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 723)               490917    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 767)               555308    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 812)               623616    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 856)               695928    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                8570      \n",
            "=================================================================\n",
            "Total params: 4,166,068\n",
            "Trainable params: 4,166,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJzPH2SZb8-J"
      },
      "source": [
        "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aI0Or8ib8-J"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V80_zdVBb8-J"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "606b85d0ba4531836f97caf6850297f8",
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5i6But7Ub8-J"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=2, first_layer_nodes=500, last_layer_nodes=100, act_funct=\"relu\", negative_node_incrementation=True)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmyLo0n4b8-K",
        "outputId": "d3e369a7-e3df-48e3-b2e4-08e842b52d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN2Bu5w6b8-K"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g33om70cb8-K"
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjsj-qg8b8-L",
        "outputId": "be7d3997-36db-425d-b753-19fc731b4dd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6561 - accuracy: 0.8051\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4439 - accuracy: 0.8680\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3594 - accuracy: 0.8928\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4661 - accuracy: 0.8656\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6672 - accuracy: 0.8004\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4544 - accuracy: 0.8650\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3646 - accuracy: 0.8903\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4540 - accuracy: 0.8687\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6627 - accuracy: 0.8023\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4507 - accuracy: 0.8655\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3615 - accuracy: 0.8923\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4679 - accuracy: 0.8642\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6320 - accuracy: 0.8060\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4307 - accuracy: 0.8680\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3457 - accuracy: 0.8936\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4686 - accuracy: 0.8641\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6422 - accuracy: 0.8041\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4361 - accuracy: 0.8667\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3467 - accuracy: 0.8935\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4339 - accuracy: 0.8727\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6303 - accuracy: 0.8077\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4237 - accuracy: 0.8690\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3393 - accuracy: 0.8955\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4537 - accuracy: 0.8668\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6571 - accuracy: 0.8040\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4449 - accuracy: 0.8690\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3597 - accuracy: 0.8923\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4677 - accuracy: 0.8666\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6659 - accuracy: 0.7999\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4536 - accuracy: 0.8647\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3648 - accuracy: 0.8927\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4555 - accuracy: 0.8654\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6642 - accuracy: 0.8003\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4509 - accuracy: 0.8651\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3593 - accuracy: 0.8921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5078 - accuracy: 0.8487\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6279 - accuracy: 0.8084\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4276 - accuracy: 0.8706\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3418 - accuracy: 0.8958\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4650 - accuracy: 0.8640\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6470 - accuracy: 0.8023\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4390 - accuracy: 0.8654\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3535 - accuracy: 0.8913\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4491 - accuracy: 0.8681\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6297 - accuracy: 0.8069\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4312 - accuracy: 0.8682\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3421 - accuracy: 0.8946\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4601 - accuracy: 0.8642\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6800 - accuracy: 0.7978\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4650 - accuracy: 0.8620\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3827 - accuracy: 0.8868\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4974 - accuracy: 0.8558\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6899 - accuracy: 0.7942\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4748 - accuracy: 0.8572\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3899 - accuracy: 0.8837\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4641 - accuracy: 0.8640\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6802 - accuracy: 0.7962\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4622 - accuracy: 0.8621\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3791 - accuracy: 0.8857\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4824 - accuracy: 0.8592\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6472 - accuracy: 0.8031\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4425 - accuracy: 0.8642\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3611 - accuracy: 0.8893\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4716 - accuracy: 0.8605\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6580 - accuracy: 0.7985\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4543 - accuracy: 0.8612\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3697 - accuracy: 0.8876\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4640 - accuracy: 0.8611\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6517 - accuracy: 0.8016\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4449 - accuracy: 0.8651\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3592 - accuracy: 0.8903\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4675 - accuracy: 0.8620\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6836 - accuracy: 0.7974\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4680 - accuracy: 0.8609\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3831 - accuracy: 0.8860\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4820 - accuracy: 0.8578\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6933 - accuracy: 0.7933\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4770 - accuracy: 0.8583\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3936 - accuracy: 0.8826\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4502 - accuracy: 0.8678\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6870 - accuracy: 0.7941\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4685 - accuracy: 0.8600\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3873 - accuracy: 0.8847\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4699 - accuracy: 0.8635\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6480 - accuracy: 0.8035\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4441 - accuracy: 0.8649\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3653 - accuracy: 0.8887\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4854 - accuracy: 0.8563\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6609 - accuracy: 0.7968\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4559 - accuracy: 0.8605\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3723 - accuracy: 0.8861\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4460 - accuracy: 0.8685\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6551 - accuracy: 0.8015\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4469 - accuracy: 0.8639\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3623 - accuracy: 0.8883\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4604 - accuracy: 0.8618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done  24 out of  24 | elapsed:  9.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 18s 8ms/step - loss: 0.5854 - accuracy: 0.8210\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.4070 - accuracy: 0.8759\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.3333 - accuracy: 0.8980\n",
            "Best: 0.8678666750590006 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8661733269691467, Stdev: 0.001867431428755251 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8678666750590006, Stdev: 0.003585203287251984 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8602133393287659, Stdev: 0.008141759437906692 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8654666741689047, Stdev: 0.00187797039551659 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8596800168355306, Stdev: 0.003367765488473004 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8611999948819479, Stdev: 0.0006101302524216542 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8630666732788086, Stdev: 0.004095045703807749 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8621866901715597, Stdev: 0.00500605568208553 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y8azbjQb8-L"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz0Ts2f9b8-L",
        "outputId": "936fc5c5-dec7-4961-c04f-efb77c0beb53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFrrHLdb8-L"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2\n",
        "\n",
        "## Benchmark different Optimization Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Random Search\n",
        "- Bayesian Optimization. \n",
        "- Brute Force Gridsearch\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which appraoch \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY90-7gab8-M"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA3m5-hsb8-M"
      },
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3QpFooQb8-M",
        "outputId": "1bdd44df-ab87-440b-bf65-5bfbe9be2186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bPDuxBFb8-M"
      },
      "source": [
        "------\n",
        "# Run the Gridsearch Algorithms \n",
        "\n",
        "### Random Search\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "uMa6Ychhb8-M"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_unique_hparam_combos = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "HsaLyMY0b8-N"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_param_combos_to_sample = 24"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Z_uKnob8-N"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHyqSXD-b8-N",
        "outputId": "91748d9f-edfa-43e0-be4a-124e08ef502c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# take note of Total elapsed time in print out\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 24 Complete [00h 00m 22s]\n",
            "val_accuracy: 0.6763200163841248\n",
            "\n",
            "Best val_accuracy So Far: 0.8701599836349487\n",
            "Total elapsed time: 00h 12m 23s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxuwavDrb8-N",
        "outputId": "9487b8e2-80d7-4a37-f33c-ab4a0163b03f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8701599836349487\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8666800260543823\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8636800050735474\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8626400232315063\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8622000217437744\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8550800085067749\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8550000190734863\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8372799754142761\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8356800079345703\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.83051997423172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "vjKqbTNEb8-N"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "N8n1Kds9b8-O"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoIvRnpFb8-O"
      },
      "source": [
        "------\n",
        "### Bayesian Optimization\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8G4QaSPb8-O"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
        "# feel free to play with any of these numbers\n",
        "max_trials=15\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nmhWYZzb8-P"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXgNuG5Kb8-P",
        "outputId": "af5d033f-cf5a-4633-868a-2c2e92b0bd80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 15 Complete [00h 00m 15s]\n",
            "val_accuracy: 0.8295199871063232\n",
            "\n",
            "Best val_accuracy So Far: 0.8747199773788452\n",
            "Total elapsed time: 00h 07m 37s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "uJnghmvdb8-P",
        "outputId": "32b8000c-16c6-4a0c-ae6d-3c3519d88db1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8747199773788452\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8690000176429749\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8580399751663208\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8578799962997437\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8329200148582458\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8303599953651428\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8295199871063232\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.828719973564148\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.8224400281906128\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8116400241851807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo15LNosb8-P"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "wjCiqaQbb8-P"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua0EC07kb8-Q"
      },
      "source": [
        "---------\n",
        "## Brute Force Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm66sK4bb8-Q"
      },
      "source": [
        "### Populate a Sklearn compatiable parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_E1h-ab8-Q"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 544, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN77-Iyub8-Q",
        "outputId": "2b06b558-ffcb-4e7e-ca04-ba457c8504a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'sigmoid'],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480,\n",
              "  512]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7LcOX4Ab8-Q"
      },
      "source": [
        "### Build a Sklearn compatiable model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrwG0fzpb8-Q"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdXUrqsTb8-R"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "SUWEPeWJb8-R",
        "outputId": "97438ec3-db23-46a6-9f7d-7a74525102a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8796 - accuracy: 0.3220\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8236 - accuracy: 0.3196\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8991 - accuracy: 0.3053\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8906 - accuracy: 0.2920\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8222 - accuracy: 0.3334\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8301 - accuracy: 0.2966\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9498 - accuracy: 0.3006\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0224 - accuracy: 0.2337\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8829 - accuracy: 0.3119\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8495 - accuracy: 0.3240\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7842 - accuracy: 0.3622\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8422 - accuracy: 0.2850\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8891 - accuracy: 0.3119\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9685 - accuracy: 0.3135\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8772 - accuracy: 0.3312\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9970 - accuracy: 0.2967\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8446 - accuracy: 0.3571\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8569 - accuracy: 0.3127\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1145 - accuracy: 0.2419\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0381 - accuracy: 0.2162\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1064 - accuracy: 0.2364\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0218 - accuracy: 0.2377\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9182 - accuracy: 0.3392\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.7783 - accuracy: 0.3256\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9591 - accuracy: 0.3272\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9095 - accuracy: 0.2766\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9834 - accuracy: 0.3171\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8706 - accuracy: 0.3065\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9246 - accuracy: 0.3289\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0152 - accuracy: 0.3151\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9592 - accuracy: 0.3182\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.2903 - accuracy: 0.2449\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.2160 - accuracy: 0.2060\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0387 - accuracy: 0.2391\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8902 - accuracy: 0.3447\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8931 - accuracy: 0.3029\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9895 - accuracy: 0.3334\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9082 - accuracy: 0.3265\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9727 - accuracy: 0.3258\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7800 - accuracy: 0.3309\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9380 - accuracy: 0.3530\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8826 - accuracy: 0.3051\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0356 - accuracy: 0.3240\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8363 - accuracy: 0.3032\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0788 - accuracy: 0.2865\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9887 - accuracy: 0.2591\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9601 - accuracy: 0.3723\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9113 - accuracy: 0.2998\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0623 - accuracy: 0.3414\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8512 - accuracy: 0.3339\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9774 - accuracy: 0.3999\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7932 - accuracy: 0.3776\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 2.0061 - accuracy: 0.4033\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8548 - accuracy: 0.3703\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1064 - accuracy: 0.3068\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.3260 - accuracy: 0.1419\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0179 - accuracy: 0.3261\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8264 - accuracy: 0.3272\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0248 - accuracy: 0.3070\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8461 - accuracy: 0.3193\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.8755 - accuracy: 0.3811\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7349 - accuracy: 0.3498\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0493 - accuracy: 0.3262\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9153 - accuracy: 0.3063\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0454 - accuracy: 0.3318\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0936 - accuracy: 0.2244\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1147 - accuracy: 0.3243\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1792 - accuracy: 0.3067\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0827 - accuracy: 0.3289\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8844 - accuracy: 0.2952\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1588 - accuracy: 0.3157\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8766 - accuracy: 0.3061\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0434 - accuracy: 0.3722\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0107 - accuracy: 0.2381\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2343 - accuracy: 0.2375\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1212 - accuracy: 0.1872\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1943 - accuracy: 0.3132\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0203 - accuracy: 0.2490\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0917 - accuracy: 0.3495\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8623 - accuracy: 0.3181\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2793 - accuracy: 0.3377\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9544 - accuracy: 0.2721\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9727 - accuracy: 0.3661\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9051 - accuracy: 0.2679\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0304 - accuracy: 0.3684\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1042 - accuracy: 0.3367\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0520 - accuracy: 0.3468\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9190 - accuracy: 0.2615\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.1253 - accuracy: 0.3340\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8818 - accuracy: 0.3154\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9988 - accuracy: 0.4154\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8195 - accuracy: 0.3516\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1520 - accuracy: 0.2973\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9398 - accuracy: 0.3272\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1850 - accuracy: 0.3698\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0865 - accuracy: 0.2924\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7968 - accuracy: 0.7553\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7143 - accuracy: 0.7932\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8139 - accuracy: 0.7528\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7011 - accuracy: 0.7862\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8168 - accuracy: 0.7527\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7099 - accuracy: 0.7862\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7439 - accuracy: 0.7755\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6911 - accuracy: 0.7874\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7666 - accuracy: 0.7642\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6797 - accuracy: 0.7946\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7585 - accuracy: 0.7690\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6469 - accuracy: 0.8005\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7412 - accuracy: 0.7746\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6688 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7522 - accuracy: 0.7698\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6617 - accuracy: 0.8038\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7470 - accuracy: 0.7733\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6698 - accuracy: 0.7980\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7447 - accuracy: 0.7734\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6836 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7483 - accuracy: 0.7731\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6574 - accuracy: 0.8095\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7430 - accuracy: 0.7754\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7045 - accuracy: 0.7912\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7355 - accuracy: 0.7794\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6830 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7491 - accuracy: 0.7727\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6832 - accuracy: 0.7930\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7307 - accuracy: 0.7777\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6777 - accuracy: 0.8006\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7356 - accuracy: 0.7772\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.8031\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7536 - accuracy: 0.7723\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6741 - accuracy: 0.7991\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7346 - accuracy: 0.7787\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6250 - accuracy: 0.8164\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7390 - accuracy: 0.7768\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6311 - accuracy: 0.8138\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7446 - accuracy: 0.7724\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6603 - accuracy: 0.8027\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7428 - accuracy: 0.7753\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6677 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7337 - accuracy: 0.7786\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6454 - accuracy: 0.8040\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7484 - accuracy: 0.7726\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.8113\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7336 - accuracy: 0.7808\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.8062\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7381 - accuracy: 0.7776\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6900 - accuracy: 0.8070\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7478 - accuracy: 0.7739\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.8041\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7444 - accuracy: 0.7763\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.8050\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7395 - accuracy: 0.7789\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6514 - accuracy: 0.8065\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7412 - accuracy: 0.7776\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6583 - accuracy: 0.8051\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7396 - accuracy: 0.7774\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6616 - accuracy: 0.8071\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7390 - accuracy: 0.7789\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7079 - accuracy: 0.7898\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7433 - accuracy: 0.7768\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6772 - accuracy: 0.7958\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7474 - accuracy: 0.7742\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6541 - accuracy: 0.8035\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7384 - accuracy: 0.7798\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6774 - accuracy: 0.8041\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7524 - accuracy: 0.7720\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6542 - accuracy: 0.8120\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7325 - accuracy: 0.7772\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6766 - accuracy: 0.8069\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7456 - accuracy: 0.7773\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7022 - accuracy: 0.7846\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7440 - accuracy: 0.7772\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6513 - accuracy: 0.8059\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7365 - accuracy: 0.7799\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6736 - accuracy: 0.8082\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7367 - accuracy: 0.7776\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6540 - accuracy: 0.8094\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7454 - accuracy: 0.7777\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6323 - accuracy: 0.8177\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7368 - accuracy: 0.7786\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6771 - accuracy: 0.8019\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7514 - accuracy: 0.7770\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7190 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7484 - accuracy: 0.7767\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6418 - accuracy: 0.8140\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7504 - accuracy: 0.7713\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6618 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.7468 - accuracy: 0.7762\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6880 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7448 - accuracy: 0.7760\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7039 - accuracy: 0.7987\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7407 - accuracy: 0.7767\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6693 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8720 - accuracy: 0.7448\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7485 - accuracy: 0.7785\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8806 - accuracy: 0.7355\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7061 - accuracy: 0.7947\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8729 - accuracy: 0.7398\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7236 - accuracy: 0.7911\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8054 - accuracy: 0.7620\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6676 - accuracy: 0.8056\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8059 - accuracy: 0.7604\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6264 - accuracy: 0.8208\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7968 - accuracy: 0.7625\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6529 - accuracy: 0.8101\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7720 - accuracy: 0.7705\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6284 - accuracy: 0.8120\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7694 - accuracy: 0.7701\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5958 - accuracy: 0.8239\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7649 - accuracy: 0.7721\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6319 - accuracy: 0.8214\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7385 - accuracy: 0.7822\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6037 - accuracy: 0.8231\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7580 - accuracy: 0.7759\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5814 - accuracy: 0.8291\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7366 - accuracy: 0.7815\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6090 - accuracy: 0.8175\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7205 - accuracy: 0.7861\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5892 - accuracy: 0.8254\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7281 - accuracy: 0.7816\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5598 - accuracy: 0.8349\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7266 - accuracy: 0.7844\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5846 - accuracy: 0.8286\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7043 - accuracy: 0.7892\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5826 - accuracy: 0.8290\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7123 - accuracy: 0.7863\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5556 - accuracy: 0.8334\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7130 - accuracy: 0.7872\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5902 - accuracy: 0.8238\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6982 - accuracy: 0.7910\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5626 - accuracy: 0.8323\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7069 - accuracy: 0.7877\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.8452\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6981 - accuracy: 0.7935\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5550 - accuracy: 0.8400\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6934 - accuracy: 0.7935\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5499 - accuracy: 0.8384\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7058 - accuracy: 0.7889\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.8389\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6902 - accuracy: 0.7949\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5551 - accuracy: 0.8348\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6857 - accuracy: 0.7957\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5557 - accuracy: 0.8352\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6960 - accuracy: 0.7934\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.8407\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6854 - accuracy: 0.7953\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5498 - accuracy: 0.8384\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6796 - accuracy: 0.7976\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5573 - accuracy: 0.8331\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6871 - accuracy: 0.7963\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5235 - accuracy: 0.8451\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6806 - accuracy: 0.7956\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5506 - accuracy: 0.8333\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6734 - accuracy: 0.7988\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5764 - accuracy: 0.8280\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6852 - accuracy: 0.7942\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5160 - accuracy: 0.8478\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6765 - accuracy: 0.7983\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5350 - accuracy: 0.8438\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6732 - accuracy: 0.7992\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5830 - accuracy: 0.8282\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6814 - accuracy: 0.7980\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5462 - accuracy: 0.8359\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6695 - accuracy: 0.7992\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5420 - accuracy: 0.8405\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6586 - accuracy: 0.8025\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.8400\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6732 - accuracy: 0.7990\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5205 - accuracy: 0.8454\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6700 - accuracy: 0.7985\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.8438\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6634 - accuracy: 0.8022\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5568 - accuracy: 0.8342\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6699 - accuracy: 0.7994\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5177 - accuracy: 0.8470\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6623 - accuracy: 0.8005\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5413 - accuracy: 0.8394\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6611 - accuracy: 0.8020\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5604 - accuracy: 0.8336\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6743 - accuracy: 0.7988\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5145 - accuracy: 0.8490\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6615 - accuracy: 0.8015\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.8458\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6588 - accuracy: 0.8033\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5358 - accuracy: 0.8402\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6655 - accuracy: 0.8000\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5018 - accuracy: 0.8543\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6594 - accuracy: 0.8027\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5286 - accuracy: 0.8409\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1806 - accuracy: 0.6177\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1460 - accuracy: 0.6178\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2760 - accuracy: 0.5777\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1619 - accuracy: 0.6333\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2340 - accuracy: 0.5902\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1240 - accuracy: 0.6246\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1912 - accuracy: 0.6239\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1206 - accuracy: 0.6639\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2034 - accuracy: 0.6240\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0737 - accuracy: 0.6822\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1517 - accuracy: 0.6312\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0416 - accuracy: 0.6578\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1710 - accuracy: 0.6330\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1418 - accuracy: 0.6507\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1858 - accuracy: 0.6254\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0068 - accuracy: 0.6654\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2086 - accuracy: 0.6247\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1307 - accuracy: 0.6350\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1879 - accuracy: 0.6290\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0373 - accuracy: 0.6782\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2061 - accuracy: 0.6295\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2007 - accuracy: 0.6456\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3047 - accuracy: 0.6164\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2329 - accuracy: 0.6401\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4034 - accuracy: 0.6031\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.6059 - accuracy: 0.6469\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2886 - accuracy: 0.6151\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0460 - accuracy: 0.6846\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2472 - accuracy: 0.6138\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2340 - accuracy: 0.6327\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2926 - accuracy: 0.6201\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1144 - accuracy: 0.6591\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2928 - accuracy: 0.6157\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1626 - accuracy: 0.6584\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3080 - accuracy: 0.6202\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2085 - accuracy: 0.6615\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3887 - accuracy: 0.6043\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1128 - accuracy: 0.6724\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1983 - accuracy: 0.6351\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1129 - accuracy: 0.6691\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4080 - accuracy: 0.6090\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3288 - accuracy: 0.6092\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3795 - accuracy: 0.5968\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2637 - accuracy: 0.6188\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5280 - accuracy: 0.5713\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2600 - accuracy: 0.6259\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4053 - accuracy: 0.5857\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2654 - accuracy: 0.5924\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4034 - accuracy: 0.6178\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.5266 - accuracy: 0.6182\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.3813 - accuracy: 0.6018\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2080 - accuracy: 0.6300\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2442 - accuracy: 0.6254\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0986 - accuracy: 0.6632\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4550 - accuracy: 0.6029\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5156 - accuracy: 0.6619\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2525 - accuracy: 0.6268\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0891 - accuracy: 0.6677\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8944 - accuracy: 0.5746\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3479 - accuracy: 0.6594\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8263 - accuracy: 0.5675\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.5971 - accuracy: 0.5770\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4835 - accuracy: 0.5914\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2901 - accuracy: 0.6420\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3856 - accuracy: 0.6199\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6948 - accuracy: 0.5892\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8942 - accuracy: 0.5710\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7176 - accuracy: 0.5861\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3851 - accuracy: 0.6123\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2927 - accuracy: 0.6401\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2767 - accuracy: 0.6203\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0872 - accuracy: 0.6592\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9100 - accuracy: 0.5754\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8374 - accuracy: 0.5837\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3983 - accuracy: 0.6301\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1488 - accuracy: 0.6764\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7720 - accuracy: 0.5747\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3527 - accuracy: 0.6442\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 1.3670 - accuracy: 0.6221\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2213 - accuracy: 0.6312\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8388 - accuracy: 0.5917\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8313 - accuracy: 0.6475\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6391 - accuracy: 0.5938\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.6593\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8425 - accuracy: 0.5760\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8955 - accuracy: 0.6143\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6577 - accuracy: 0.5915\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2957 - accuracy: 0.6737\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3474 - accuracy: 0.6210\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2211 - accuracy: 0.6484\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8017 - accuracy: 0.5653\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8289 - accuracy: 0.5776\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7549 - accuracy: 0.5787\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.5056 - accuracy: 0.6209\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6253 - accuracy: 0.5829\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.3787 - accuracy: 0.6351\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7860 - accuracy: 0.7576\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7050 - accuracy: 0.7865\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7997 - accuracy: 0.7557\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7095 - accuracy: 0.7769\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7954 - accuracy: 0.7578\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7099 - accuracy: 0.7802\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7270 - accuracy: 0.7771\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6404 - accuracy: 0.8091\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7413 - accuracy: 0.7730\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6320 - accuracy: 0.8050\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7341 - accuracy: 0.7737\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6309 - accuracy: 0.8114\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7024 - accuracy: 0.7846\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6268 - accuracy: 0.8120\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7183 - accuracy: 0.7805\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6234 - accuracy: 0.8094\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7061 - accuracy: 0.7831\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6211 - accuracy: 0.8141\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6971 - accuracy: 0.7849\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6189 - accuracy: 0.8129\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7105 - accuracy: 0.7820\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6377 - accuracy: 0.8004\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7033 - accuracy: 0.7828\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6116 - accuracy: 0.8178\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6864 - accuracy: 0.7884\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6119 - accuracy: 0.8112\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6998 - accuracy: 0.7848\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.8235\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6965 - accuracy: 0.7862\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.8100\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6851 - accuracy: 0.7887\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6561 - accuracy: 0.7964\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6911 - accuracy: 0.7887\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5918 - accuracy: 0.8168\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6865 - accuracy: 0.7901\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5896 - accuracy: 0.8202\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6812 - accuracy: 0.7888\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6171 - accuracy: 0.8145\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7013 - accuracy: 0.7840\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6373 - accuracy: 0.8019\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6891 - accuracy: 0.7897\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.8209\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6815 - accuracy: 0.7914\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6391 - accuracy: 0.8056\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6903 - accuracy: 0.7866\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5742 - accuracy: 0.8252\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6804 - accuracy: 0.7911\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5923 - accuracy: 0.8212\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6881 - accuracy: 0.7869\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6219 - accuracy: 0.8180\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6918 - accuracy: 0.7868\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6277 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6840 - accuracy: 0.7896\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6009 - accuracy: 0.8218\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6814 - accuracy: 0.7894\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6153 - accuracy: 0.8144\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6986 - accuracy: 0.7833\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6172 - accuracy: 0.8104\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6854 - accuracy: 0.7886\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5960 - accuracy: 0.8188\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6745 - accuracy: 0.7910\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5834 - accuracy: 0.8245\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6918 - accuracy: 0.7867\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5789 - accuracy: 0.8274\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6847 - accuracy: 0.7898\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5898 - accuracy: 0.8246\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6846 - accuracy: 0.7893\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5979 - accuracy: 0.8246\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6956 - accuracy: 0.7891\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5825 - accuracy: 0.8248\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6844 - accuracy: 0.7892\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5971 - accuracy: 0.8174\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6892 - accuracy: 0.7874\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6042 - accuracy: 0.8197\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6947 - accuracy: 0.7852\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5739 - accuracy: 0.8235\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6908 - accuracy: 0.7871\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6033 - accuracy: 0.8142\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6945 - accuracy: 0.7880\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.6049 - accuracy: 0.8164\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6974 - accuracy: 0.7867\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6276 - accuracy: 0.8127\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6905 - accuracy: 0.7899\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5885 - accuracy: 0.8225\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6834 - accuracy: 0.7897\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6034 - accuracy: 0.8181\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6938 - accuracy: 0.7856\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5684 - accuracy: 0.8293\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6928 - accuracy: 0.7889\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6125 - accuracy: 0.8170\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6898 - accuracy: 0.7897\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5919 - accuracy: 0.8235\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6968 - accuracy: 0.7867\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6016 - accuracy: 0.8199\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6903 - accuracy: 0.7849\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5884 - accuracy: 0.8229\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0346 - accuracy: 0.7085\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8267 - accuracy: 0.7554\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0443 - accuracy: 0.7034\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7951 - accuracy: 0.7632\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0513 - accuracy: 0.6987\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8194 - accuracy: 0.7556\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9268 - accuracy: 0.7326\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7612 - accuracy: 0.7768\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9310 - accuracy: 0.7326\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7332 - accuracy: 0.7832\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9357 - accuracy: 0.7291\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7514 - accuracy: 0.7803\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8915 - accuracy: 0.7387\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7339 - accuracy: 0.7853\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9077 - accuracy: 0.7347\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7101 - accuracy: 0.7881\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8966 - accuracy: 0.7366\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7263 - accuracy: 0.7890\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8779 - accuracy: 0.7389\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7292 - accuracy: 0.7810\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8752 - accuracy: 0.7431\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6914 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8695 - accuracy: 0.7444\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7159 - accuracy: 0.7913\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8553 - accuracy: 0.7469\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7183 - accuracy: 0.7851\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8610 - accuracy: 0.7442\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8596 - accuracy: 0.7461\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7152 - accuracy: 0.7906\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8459 - accuracy: 0.7491\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7097 - accuracy: 0.7912\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8482 - accuracy: 0.7496\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6789 - accuracy: 0.7990\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8454 - accuracy: 0.7488\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6945 - accuracy: 0.7951\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8325 - accuracy: 0.7538\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7028 - accuracy: 0.7943\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8494 - accuracy: 0.7477\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6702 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8393 - accuracy: 0.7525\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6894 - accuracy: 0.7968\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8309 - accuracy: 0.7548\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6971 - accuracy: 0.7959\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8392 - accuracy: 0.7513\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6679 - accuracy: 0.8044\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8372 - accuracy: 0.7510\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6842 - accuracy: 0.8001\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8259 - accuracy: 0.7519\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7029 - accuracy: 0.7905\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8338 - accuracy: 0.7541\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.8029\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8274 - accuracy: 0.7553\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6795 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8200 - accuracy: 0.7571\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6906 - accuracy: 0.7958\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8337 - accuracy: 0.7528\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6517 - accuracy: 0.8108\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8318 - accuracy: 0.7526\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6899 - accuracy: 0.7977\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8154 - accuracy: 0.7579\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.7960\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8300 - accuracy: 0.7530\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6604 - accuracy: 0.8041\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8209 - accuracy: 0.7566\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6998 - accuracy: 0.7905\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8127 - accuracy: 0.7574\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8304 - accuracy: 0.7538\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6650 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8205 - accuracy: 0.7557\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6737 - accuracy: 0.8022\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8112 - accuracy: 0.7584\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7008 - accuracy: 0.7915\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8264 - accuracy: 0.7528\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6608 - accuracy: 0.8027\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8166 - accuracy: 0.7565\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6748 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8116 - accuracy: 0.7584\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6988 - accuracy: 0.7926\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8244 - accuracy: 0.7538\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6452 - accuracy: 0.8102\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8163 - accuracy: 0.7575\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6804 - accuracy: 0.7978\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8067 - accuracy: 0.7590\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6726 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8182 - accuracy: 0.7546\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6548 - accuracy: 0.8078\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8169 - accuracy: 0.7555\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6645 - accuracy: 0.8058\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8057 - accuracy: 0.7591\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6754 - accuracy: 0.8005\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8163 - accuracy: 0.7561\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6389 - accuracy: 0.8109\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8099 - accuracy: 0.7580\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6801 - accuracy: 0.7959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done 288 out of 288 | elapsed: 43.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2344/2344 [==============================] - 12s 5ms/step - loss: 0.6111 - accuracy: 0.8179\n",
            "Best: 0.8451199928919474 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
            "Means: 0.30269332726796466, Stdev: 0.012073476208811966 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.28089333573977154, Stdev: 0.036954593509803776 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.3076266646385193, Stdev: 0.0077190899457129335 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.259813333551089, Stdev: 0.047340550463653305 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.299426664908727, Stdev: 0.016490689421426792 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.26230667034784955, Stdev: 0.02881676770921052 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.320853332678477, Stdev: 0.01126923705995397 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.2873733341693878, Stdev: 0.02002858419158406 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.36058666308720905, Stdev: 0.019114366139374426 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.2628133296966553, Stdev: 0.08554623772646117 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.2935066670179367, Stdev: 0.05195775272607881 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.30266666412353516, Stdev: 0.005286196226709301 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.22480000058809915, Stdev: 0.02693048474259848 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.28603999813397724, Stdev: 0.022720500283997507 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.30454666415850323, Stdev: 0.031636000984715786 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.323746661345164, Stdev: 0.024326685842590366 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.7885199983914694, Stdev: 0.0033093124823737976 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.7941333452860514, Stdev: 0.005364706050313921 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8011999924977621, Stdev: 0.002426743278190414 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8014266689618429, Stdev: 0.007641115631754951 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.7962133288383484, Stdev: 0.0032033888761548975 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8061866760253906, Stdev: 0.007380277651652415 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8076133330663046, Stdev: 0.0046040234751731045 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.807146688302358, Stdev: 0.0030624257070311386 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8053999940554301, Stdev: 0.0012189489509251536 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8062399824460348, Stdev: 0.0008242900378755545 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.7963599960009257, Stdev: 0.005631449166360664 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8076933224995931, Stdev: 0.003279227560705583 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.799560010433197, Stdev: 0.010620627344765574 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8096400101979574, Stdev: 0.006453360198872245 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8055733442306519, Stdev: 0.006762281677689685 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7984266678492228, Stdev: 0.00021745816329164802 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7880799969037374, Stdev: 0.00694550471757575 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.812173326810201, Stdev: 0.006341186921241835 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8191199898719788, Stdev: 0.005110944329242312 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8232400019963583, Stdev: 0.004752748341942189 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.8296399911244711, Stdev: 0.00393426048287387 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8287333250045776, Stdev: 0.003956352996800302 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8391600052515665, Stdev: 0.005294941354954882 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8373600045839945, Stdev: 0.0018245720115811874 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8381066719690958, Stdev: 0.0022305342927048197 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8371600111325582, Stdev: 0.005629163511136015 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8398666779200236, Stdev: 0.008548426310896575 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8348533312479655, Stdev: 0.005097916755481397 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.84306667248408, Stdev: 0.002232907065945138 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8402133186658224, Stdev: 0.005286606085539703 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8427733182907104, Stdev: 0.006616773076685921 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.8451199928919474, Stdev: 0.006484525836306855 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
            "Means: 0.6252533197402954, Stdev: 0.0063182504417849655 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.6679866711298624, Stdev: 0.010375887678488462 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.6503866712252299, Stdev: 0.012396676561877849 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.654639999071757, Stdev: 0.016813367192038766 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.6547466516494751, Stdev: 0.021913998773042277 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6596800088882446, Stdev: 0.001333865545397416 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6502666672070821, Stdev: 0.029041882508339244 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.6123600006103516, Stdev: 0.014431214122747657 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.6370933453241984, Stdev: 0.019051051268187975 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6630000074704488, Stdev: 0.003497362088532619 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6027200222015381, Stdev: 0.028196133034703304 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6284666856129965, Stdev: 0.030967551221792986 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.6347600022951762, Stdev: 0.03841583124838601 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.6459866762161255, Stdev: 0.01153102081079805 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.6454533338546753, Stdev: 0.02432063325094909 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.6111999948819479, Stdev: 0.024459678259042893 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.7812000115712484, Stdev: 0.003987574399764019 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.8085333307584127, Stdev: 0.0026455119591403905 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8118533293406168, Stdev: 0.0019314350090213202 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8103866577148438, Stdev: 0.0073100582193148 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8149066766103109, Stdev: 0.006112919240440339 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8111200133959452, Stdev: 0.01052879203017198 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8124133348464966, Stdev: 0.007893173865521543 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8173200090726217, Stdev: 0.008443522394425732 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8148533503214518, Stdev: 0.007248444246162873 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8145466645558676, Stdev: 0.003430099830519219 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.825493335723877, Stdev: 0.0013483344019554913 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8222266634305319, Stdev: 0.0034422309050054683 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.819106658299764, Stdev: 0.0038264183266719727 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8172000050544739, Stdev: 0.004024468860045951 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.821453332901001, Stdev: 0.005554528125365028 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8220800161361694, Stdev: 0.001574805523232973 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7580533226331075, Stdev: 0.0036684289414448145 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.780079980691274, Stdev: 0.002600305908222608 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.7874799768129984, Stdev: 0.0015690290321422977 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7898933291435242, Stdev: 0.00676667893061459 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7922400037447611, Stdev: 0.006564874684364562 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7950933376948038, Stdev: 0.0032170234301022766 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7973600029945374, Stdev: 0.002771859744078868 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8001066644986471, Stdev: 0.003461977577119363 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.7975866595904032, Stdev: 0.005199406892994884 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.801466683546702, Stdev: 0.006672232826450915 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.7968666752179464, Stdev: 0.005569663048169619 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8008933266003927, Stdev: 0.0011704670611986851 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.7983866731325785, Stdev: 0.00493632494602187 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8001599907875061, Stdev: 0.007382859917394645 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8048933347066244, Stdev: 0.002826985357474648 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.8024133443832397, Stdev: 0.0062744579859604285 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnkdAMFRb8-R",
        "outputId": "6404849b-a478-46d4-d86a-a71350f183fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_miniutes = (end - start)/60\n",
        "total_run_time_in_miniutes"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43.3483989238739"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPBtvPUfb8-R",
        "outputId": "900ffabc-a10a-402d-cf5e-b9bffaf98690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 512}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4nXOk7Ob8-R",
        "outputId": "5e24c735-71ae-43a9-b175-cb1fe7c5ea83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5052 - accuracy: 0.8539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udCKjpHub8-S",
        "outputId": "e0c330c0-0b51-4efb-aedb-eedc9f37be67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.853879988193512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmjlXhuyb8-S"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "DW_O1raqb8-S"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP6npDpyb8-S"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXUCsjBAb8-S"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridserach experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEiSQqdAb8-T"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}